# End-to-End Data Engineering Project: dbt, Snowflake & Apache Airflow

## ğŸ“Œ Overview
This project is a complete data engineering pipeline using **dbt**, **Snowflake**, and **Apache Airflow**.  
It covers data ingestion, transformation, and workflow scheduling in a structured, scalable way.

---

## ğŸ›  Tech Stack
- **dbt Core** â€“ Data transformation & modeling  
- **Snowflake** â€“ Cloud-based data warehouse  
- **Apache Airflow** â€“ Workflow automation & orchestration  
- **Python** â€“ Scripting & automation  
- **Git** â€“ Version control  

---

## ğŸ“ Project Structure

```
snowflake_dataeng_project/
â”‚â”€â”€ models/                 # dbt models (staging, marts)
â”‚â”€â”€ dags/                   # Airflow DAGs (for scheduling)
â”‚â”€â”€ logs/                   # Airflow logs
â”‚â”€â”€ seeds/                  # Seed data for dbt
â”‚â”€â”€ macros/                 # dbt macros
â”‚â”€â”€ dbt_project.yml         # dbt project config file
â”‚â”€â”€ README.md               # Project documentation
```

---

## ğŸ§° Set Up a Virtual Environment

**Mac / Linux**
```bash
python -m venv venv
source venv/bin/activate
```

**Windows**
```powershell
python -m venv venv
venv\Scripts\activate
```

---

## â„ï¸ Configure dbt Connection to Snowflake

Update the `profiles.yml` file located in:

- **Mac/Linux:** `~/.dbt/profiles.yml`  
- **Windows:** `C:\Users\<username>\.dbt\profiles.yml`

Use this configuration:

```yaml
snowflake_project:
  outputs:
    dev:
      account: your_snowflake_account
      database: finance_db
      user: dbt_user
      password: your_password
      warehouse: finance_wh
      role: ACCOUNTADMIN
      schema: raw
      type: snowflake
  target: dev
```

---

## ğŸ— Run dbt Models

```bash
dbt run
dbt test        # Validates data integrity
```

---

## ğŸš€ Start Apache Airflow

```bash
airflow standalone        # Starts the UI & Scheduler
```

---
